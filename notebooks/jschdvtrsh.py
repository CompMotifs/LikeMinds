# -*- coding: utf-8 -*-
"""Like_extraction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yxhkk6BSYDeLEKNCmXGffl2XcmSxKEc8
"""

def get_multiple_profiles_likes_df(profile_ids, total_posts_per_profile, include_text=False, max_workers=5):
    """
    Get likes from multiple Bluesky profiles and return as a combined pandas DataFrame

    Parameters:
    profile_ids (list): List of Bluesky handles or DIDs
    total_posts_per_profile (int): Number of posts to extract per profile
    include_text (bool): Whether to include post text (default False)
    max_workers (int): Maximum number of concurrent requests (default 5)

    Returns:
    DataFrame: Pandas DataFrame with liked posts and their details
    """
    all_dfs = []

    # Function to process a single profile
    def process_profile(profile_id):
        config = {
            "profile_id": profile_id,
            "total_posts": total_posts_per_profile,
            "include_text": include_text,
            "text_preview_only": True,  # Only need previews for combined view
            "rate_limit_delay": 1
        }

        try:
            df = get_likes_df(config)
            # Add profile_id column
            df["profile_id"] = profile_id
            return df
        except Exception as e:
            print(f"Error processing {profile_id}: {e}")
            return pd.DataFrame()  # Return empty DataFrame on error

    # Process profiles concurrently with rate limiting
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        results = list(executor.map(process_profile, profile_ids))

    # Combine all results
    combined_df = pd.concat(results, ignore_index=True)

    # Select only the needed columns
    if include_text:
        columns = ["profile_id", "uri", "url", "author", "author_handle", "liked_at", "text_preview"]
    else:
        columns = ["profile_id", "uri", "url", "author", "author_handle", "liked_at"]

    return combined_df[columns]

import requests
import json
import time
import pandas as pd
from concurrent.futures import ThreadPoolExecutor

def get_did_from_handle(handle):
    """Convert a Bluesky handle to a DID"""
    response = requests.get(
        "https://public.api.bsky.app/xrpc/com.atproto.identity.resolveHandle",
        params={"handle": handle}
    )
    if not response.ok:
        raise Exception(f"Failed to resolve handle: {response.status_code}")
    return response.json()["did"]

def get_service_endpoint(did):
    """Get the PDS service endpoint for a DID"""
    if did.startswith('did:web:'):
        response = requests.get(f"https://{did[8:]}/.well-known/did.json")
    else:
        response = requests.get(f"https://plc.directory/{did}")

    if not response.ok:
        raise Exception(f"Failed to get DID info: {response.status_code}")

    did_info = response.json()
    if not did_info.get("service") or not did_info["service"][0]:
        raise Exception("Could not find service endpoint")

    return did_info["service"][0]["serviceEndpoint"]

def get_likes_df(config):
    """
    Get likes from a Bluesky profile and return as a pandas DataFrame

    Parameters:
    config (dict): Configuration dictionary with the following keys:
        - profile_id: Bluesky handle or DID (required)
        - total_posts: Total number of posts to extract (default 25)
        - include_text: Whether to include post text (default True)
        - text_preview_only: If True, only include preview in DataFrame (default False)
        - preview_length: Number of characters for text preview (default 20)
        - rate_limit_delay: Time to wait between requests in seconds (default 1)

    Returns:
    DataFrame: Pandas DataFrame with liked posts and their details
    """
    # Extract config parameters with defaults
    profile_id = config.get("profile_id")
    total_posts = config.get("total_posts", 25)
    include_text = config.get("include_text", True)
    text_preview_only = config.get("text_preview_only", False)
    preview_length = config.get("preview_length", 20)
    rate_limit_delay = config.get("rate_limit_delay", 1)

    if not profile_id:
        raise ValueError("profile_id is required")

    # Convert handle to DID if needed
    did = profile_id if profile_id.startswith('did:') else get_did_from_handle(profile_id)

    # Get the PDS endpoint
    endpoint = get_service_endpoint(did)

    all_likes = []
    cursor = None
    limit_per_page = min(100, total_posts)  # Max 100 per API limitation

    # Paginate through likes until we have enough or there are no more
    while len(all_likes) < total_posts:
        # Set up request parameters
        params = {
            "repo": did,
            "collection": "app.bsky.feed.like",
            "limit": min(limit_per_page, total_posts - len(all_likes))
        }
        if cursor:
            params["cursor"] = cursor

        # Make the request
        response = requests.get(
            f"{endpoint}/xrpc/com.atproto.repo.listRecords",
            params=params
        )
        if not response.ok:
            raise Exception(f"Failed to get likes: {response.status_code}")

        likes_data = response.json()

        # Extract post URIs from likes
        post_uris = []
        new_likes = []

        for like_record in likes_data.get("records", []):
            subject = like_record.get("value", {}).get("subject", {})
            uri = subject.get("uri", "")
            # Only include app.bsky.feed.post URIs
            if "/app.bsky.feed.post/" in uri:
                post_uris.append(uri)
                post_info = extract_post_info(uri)
                new_likes.append({
                    "uri": uri,
                    "liked_at": like_record.get("value", {}).get("createdAt"),
                    "url": post_info["url"],
                    "author": post_info["repo"]
                })

        # Get post details if requested
        if include_text and post_uris:
            posts_details = get_post_details(post_uris)

            # Add post text to like records
            for like in new_likes:
                post_detail = next((p for p in posts_details if p["uri"] == like["uri"]), None)
                if post_detail:
                    if "record" in post_detail and "text" in post_detail["record"]:
                        like["text"] = post_detail["record"]["text"]
                    if "author" in post_detail:
                        like["author_handle"] = post_detail["author"].get("handle", "")
                        like["author_display_name"] = post_detail["author"].get("displayName", "")
                    like["repost_count"] = post_detail.get("repostCount", 0)
                    like["like_count"] = post_detail.get("likeCount", 0)
                    like["reply_count"] = post_detail.get("replyCount", 0)
                else:
                    like["text"] = ""

        all_likes.extend(new_likes)

        # Check if there are more records
        cursor = likes_data.get("cursor")
        if not cursor:
            break

        # Respect rate limits
        time.sleep(rate_limit_delay)

    # Trim to the exact number requested
    all_likes = all_likes[:total_posts]

    # Convert to DataFrame
    df = pd.DataFrame(all_likes)

    # Add text preview column
    if include_text and "text" in df.columns:
        df["text_preview"] = df["text"].apply(lambda x: (x[:preview_length] + "...") if len(x) > preview_length else x)

        # If preview only, drop the full text column
        if text_preview_only:
            df = df.drop(columns=["text"])

    return df

def extract_post_info(uri):
    """Get basic info about a post from its URI"""
    parts = uri.split("/")
    return {
        "repo": parts[2],
        "collection": parts[3],
        "rkey": parts[4],
        "url": f"https://bsky.app/profile/{parts[2]}/post/{parts[4]}"
    }

def get_post_details(post_uris):
    """Get detailed information about posts from their URIs"""
    if not post_uris:
        return []

    # API might have a limit on the number of URIs per request
    # Split into chunks of 25 if needed
    max_uris_per_request = 25
    all_posts = []

    for i in range(0, len(post_uris), max_uris_per_request):
        chunk = post_uris[i:i + max_uris_per_request]
        response = requests.get(
            "https://public.api.bsky.app/xrpc/app.bsky.feed.getPosts",
            params={"uris": chunk}
        )
        if not response.ok:
            raise Exception(f"Failed to get posts: {response.status_code}")

        all_posts.extend(response.json().get("posts", []))

    return all_posts

# Example for a single profile
single_config = {
    "profile_id": "achterbrain.bsky.social",  # Example handle
    "total_posts": 10,
    "include_text": True,
    "text_preview_only": False,
    "preview_length": 30,
    "rate_limit_delay": 1
}

try:
    # Single profile example
    df = get_likes_df(single_config)
    print(f"Successfully extracted {len(df)} likes from a single profile")
    print("\nDataFrame columns:", df.columns.tolist())
    print("\nPreview of single profile data:")
    print(df[["url", "liked_at", "text_preview"]].head(3))

    # Multiple profiles example
    profile_ids = ["achterbrain.bsky.social", "compmotifs.bsky.social"]
    combined_df = get_multiple_profiles_likes_df(profile_ids, total_posts_per_profile=5, include_text=True)
    print(f"\nSuccessfully extracted {len(combined_df)} likes from {len(profile_ids)} profiles")
    print("\nPreview of combined profile data:")
    print(combined_df[["profile_id", "url", "liked_at"]].head(5))
except Exception as e:
    print(f"Error: {e}")

df.iloc[1]['text']

combined_df

